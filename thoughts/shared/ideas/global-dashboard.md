# Claude Conversation Export
*Exported on 10/12/2025, 14:57:38*

---

## üë§ User

The serena MCP spins up a new "instance" for every IDe window, claude code conversation and claude code desktop app launch. Each has a python process (e.g. process ID 24284 currently running, which you may feel free to inspect using command line commands). There is also a dashboard shown for each instance (e.g. "http://127.0.0.1:24284/dashboard/index.html").

its very annoying to have instancing spinning up constantly and when a vscode window or claude code chat is closed the instances are not cleaned up. 

I want you to check out the source code "https://github.com/oraios/serena" and give me some ideas for a centralised core server (instead of one server per project) OR enhance the dashboard to show and manage all the instances on all ports (and only launch the dashboard once). Think long and hard about various ways to do this, feel free to think outside the box and suggest alternative solutions. In an ideal worl it would be nice if there was only one serena instance on one port and all projects connected to it but thats likely not feasible (too big an architecture shift)

---

## ü§ñ Assistant

You‚Äôre not wrong, Mick ‚Äì the way Serena is wired right now *naturally* leads to ‚ÄúPython-process confetti‚Äù all over your machine.

From the code + docs:

- `serena start-mcp-server` always spins up a full MCP server + SerenaAgent, and in your config the agent usually starts a web dashboard in-process.
- Each MCP client (Claude Code, each IDE window, Claude Desktop session, etc.) typically configures Serena as a *stdio* server, so every client spawn = new process.
- The dashboard is a Flask app (`SerenaDashboardAPI`) that binds to the first free port starting at 24282 (0x5EDA) and exposes logs + a `/shutdown` endpoint.
- `SerenaAgent` already supports *multiple projects* internally (config holds a list of projects, and `activate_project` tools exist).
- Since v0.1.3, the **MCP server is the only asyncio component**, and process isolation is explicitly *no longer required* for stability.

So: the ‚Äúone process per window‚Äù pattern is mostly driven by *how clients wire it up* (stdio + per-project command), not by core Serena limitations.

Let me lay out options from ‚Äúeasy and realistic now‚Äù through to ‚Äúbig architecture shift‚Äù, and then tell you what I‚Äôd actually implement first.

---

## 0. Quick knobs you can turn today

These don‚Äôt fix the process explosion, but they *do* remove the worst annoyance (dashboards everywhere) and give you manual control.

### 0.1 Stop the dashboard popping up for every instance

Config and CLI already support this:

- In `~/.serena/serena_config.yml` (or the project-local `.serena/serena_config.yml`), set:

```yaml
web_dashboard: true              # or false if you really hate it
web_dashboard_open_on_launch: false

```

- Or override per client:

```bash
serena start-mcp-server --enable-web-dashboard false
# or leave it enabled but:
serena start-mcp-server --enable-web-dashboard true --log-level INFO
# and rely on web_dashboard_open_on_launch: false

```

That keeps the dashboard available on `http://localhost:24282/dashboard/index.html` (or 24283, 24284‚Ä¶) but stops Serena from auto-launching the browser tab every time.

### 0.2 Use the dashboard as a manual ‚Äúprocess killer‚Äù

The current dashboard already has a shutdown button wired to `/shutdown`, which calls back into `SerenaDashboardAPI` and exits the process.

So one pragmatic pattern is:

- Disable auto-open.
- When you notice things piling up, open `http://localhost:24282/dashboard/index.html`, bump ports until you see all your recent instances, hit ‚ÄúShutdown‚Äù on the ones you don‚Äôt care about.

Crude, but no code changes.

---

## 1. Better multi-instance management without a central server

This is the smallest *code* change that directly hits your pain:

> Enhance the dashboard to show and manage all the instances on all ports and only launch the dashboard once.

You can get **90% of that** by adding an *instance registry* and very small changes to `agent.py` + `dashboard.py` + `dashboard.js`.

### 1.1 Add an InstanceRegistry (global state for running Serena instances)

Create something like `src/serena/util/instance_registry.py`:

- Stores one JSON file under `SERENA_MANAGED_DIR_IN_HOME`, e.g. `~/.serena/instances.json`.
- Each record: 
```json
{
  "pid": 24284,
  "port": 24284,
  "project_name": "my-service",
  "project_root": "/home/mick/dev/my-service",
  "context": "ide-assistant",
  "modes": ["default"],
  "started_at": 1733822312.123,
  "last_seen": 1733822312.123
}

```
- API surface: 
- `register(info: InstanceInfo) -> None`
- `touch(pid) -> None` (update `last_seen`)
- `deregister(pid) -> None`
- `list(filter_stale: bool = True) -> list[InstanceInfo]`

You can optionally prune ‚Äúobviously dead‚Äù entries by checking `os.kill(pid, 0)` on Unix / `psutil` if you‚Äôre willing to add that dependency; otherwise just leave a bit of cruft.

### 1.2 Register & deregister instances inside SerenaAgent and dashboard

In `SerenaAgent.__init__` (where the dashboard is started):

- After:

```python
self._dashboard_thread, port = SerenaDashboardAPI(...).run_in_thread()
dashboard_url = f"http://127.0.0.1:{port}/dashboard/index.html"

```

- Add something like:

```python
from serena.util.instance_registry import InstanceRegistry, InstanceInfo

registry = InstanceRegistry()
registry.register(
    InstanceInfo(
        pid=os.getpid(),
        port=port,
        project_name=self._active_project.project_name if self._active_project else None,
        project_root=self._active_project.project_root if self._active_project else None,
        context=self._context.name,
        modes=[m.name for m in self._modes],
    )
)

```

In `SerenaDashboardAPI._setup_routes` / `/shutdown`:

- Before calling `_shutdown_callback()` or `os._exit(0)`, call `InstanceRegistry().deregister(os.getpid())`.

You might also:

- Update `last_seen` in a cheap place (e.g. in the API handler that returns logs), so the registry has a reasonably fresh timestamp.

### 1.3 Extend the dashboard API with /instances and /instances/<pid>/shutdown

In `SerenaDashboardAPI._setup_routes`:

- Add:

```python
@app.route("/instances", methods=["GET"])
def list_instances():
    registry = InstanceRegistry()
    return jsonify([i.to_json_dict() for i in registry.list()])

```

- Add a management endpoint:

```python
@app.route("/instances/<int:pid>/shutdown", methods=["POST"])
def shutdown_instance(pid: int):
    registry = InstanceRegistry()
    inst = registry.get(pid)
    if inst is None:
        return jsonify({"ok": False, "error": "Unknown instance"}), 404

    # Call that instance's own /shutdown endpoint
    try:
        import urllib.request, json
        req = urllib.request.Request(
            f"http://127.0.0.1:{inst.port}/shutdown",
            data=b"",
            method="POST",
        )
        urllib.request.urlopen(req, timeout=2)
        return jsonify({"ok": True})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

```

Now *any* dashboard instance can see and control *all* others.

### 1.4 Update the dashboard UI to act as a ‚Äúglobal manager‚Äù

`src/serena/resources/dashboard/index.html` + `dashboard.js` already:

- Load logs
- Show usage charts
- Invoke `/shutdown` for the *current* instance.

You extend that with:

- A table/dropdown of instances fetched from `/instances` (PID, project, context, port, last activity).
- Buttons: 
- ‚ÄúOpen in browser‚Äù ‚Üí `window.open('http://127.0.0.1:' + port + '/dashboard/index.html')`
- ‚ÄúShutdown‚Äù ‚Üí POST `/instances/<pid>/shutdown`
- Visually mark the ‚Äúcurrent‚Äù instance (matching `window.location.port`).

Effectively you get:

- **Single dashboard tab** you keep pinned.
- From there, you see *all* Serena processes and can kill / inspect them as needed.

### 1.5 Only open the dashboard once

Simple heuristic on top of the registry:

- When `SerenaAgent` starts and `web_dashboard_open_on_launch` is true: 
- Check the registry: if there is *any* other instance with `web_dashboard_open_on_launch_used: true` in the last N minutes, skip auto-open.
- Otherwise, open the browser and mark this instance as the one that actually opened it.

Implementation detail: either add a small boolean to `InstanceInfo`, or keep a separate `~/.serena/dashboard_lock` file with a timestamp.

This is low risk, backward compatible, and doesn‚Äôt touch MCP semantics at all.

---

## 2. Reducing zombie processes: auto-shutdown / idle timeout

You also mentioned that instances are **not cleaned up** when the IDE window / Claude chat is closed.

This is largely because:

- Clients spawn Serena as a subprocess and frequently *don‚Äôt* send a clean shutdown or kill it on teardown.

Serena can defend itself with a simple **idle timeout**.

### 2.1 Add idle_shutdown_minutes to SerenaConfig

In the config base (see the template around `web_dashboard`, `tool_timeout`, etc.):

```python
idle_shutdown_minutes: int | None = None  # None = disabled, 0 not allowed

```

Expose that in:

- `serena_config.yml` template.
- CLI `start-mcp-server` as `--idle-shutdown-minutes` override.

### 2.2 Track ‚Äúlast activity‚Äù and kill on timeout

The natural place is `SerenaMCPFactorySingleProcess` / `server_lifespan`.

- Wrap all tool calls to update `last_activity = time.time()`.
- Spawn a background thread:

```python
def _idle_watchdog():
    while True:
        time.sleep(60)
        if idle_shutdown_minutes and time.time() - last_activity > idle_shutdown_minutes * 60:
            log.info("Idle timeout reached, shutting down Serena")
            os._exit(0)

```

This won‚Äôt perfectly align with actual MCP connection lifecycle, but:

- Any real use (tool call) resets the timer.
- ‚ÄúForgotten‚Äù instances quietly die after e.g. 30 minutes of inactivity.

Combined with the instance registry, your dashboard can even show ‚ÄúIdle since X min; will auto-shutdown soon‚Äù.

---

## 3. A real centralised core server (single process, many clients)

Now to your ‚Äúideal world‚Äù idea: one Serena instance on one port, all projects connect to it.

### 3.1 The big constraint: stdio vs HTTP

- **STDIO transport** is 1:1 by design: one process <-> one client, via stdin/stdout pipes. You *cannot* have multiple Claude Code windows sharing one stdio process without some fairly cursed multiplexing.
- **HTTP / Streamable HTTP transports** are *explicitly* designed for multi-client servers.

Claude Code **does** support HTTP/streamable-http transports now:

```bash
# remote / local HTTP MCP
claude mcp add --transport http serena http://127.0.0.1:8000/mcp

```

And Serena‚Äôs `start-mcp-server` already supports `--transport streamable-http` with host/port.

So the path of least resistance to ‚Äúone process‚Äù is:

1. **Run Serena once** as a long-lived HTTP/streamable-http MCP server, e.g.: 
```bash
uvx --from git+https://github.com/oraios/serena \
    serena-mcp-server \
    --transport streamable-http \
    --host 127.0.0.1 \
    --port 9121 \
    --context ide-assistant

```
2. **Point Claude Code & other clients at that HTTP endpoint** instead of spawning stdio processes: 
```bash
claude mcp add --transport http serena http://127.0.0.1:9121/mcp

```
3. **Use Serena‚Äôs built-in multi-project mechanisms** to handle different repos: 
- `SerenaConfig` already tracks a list of projects and supports activation by name or path.
- There‚Äôs a tool `ActivateProjectTool` exposed in non-IDE contexts that can activate arbitrary projects at runtime.
 For HTTP clients, your LLM can just *call that tool* and set the active project based on the path / name it‚Äôs working with.

The only real awkwardness is **per-client isolation**: one shared agent + one shared active project means:

- If two Claude Code instances talk to the same Serena HTTP server, they‚Äôd step on each other‚Äôs ‚Äúactive project‚Äù unless you introduce a **session or workspace ID** concept.

### 3.2 Multi-session Serena: per-connection project state

This is the more invasive step, but still very much possible within the current architecture.

Idea:

- Extend the MCP layer (in `SerenaMCPFactory` / `SerenaMCPRequestContext`) to track a **session ID per client**.
- Change `SerenaAgent` so that: 
- Instead of a single `_active_project`, it maintains a mapping `session_id -> active_project`.
- Tools that currently use `get_active_project()` take a `session_id` and operate in that context.

This requires:

- A small custom `FastMCP` ‚Äúrequest context‚Äù hook (fastmcp already provides some context mechanism; you‚Äôd need to feed a per-request/connection identifier into `SerenaMCPRequestContext`).
- Modifying tool implementations to take session context instead of assuming a single global active project.

It‚Äôs a non-trivial refactor but *architecturally clean*: one process, one LSP pool, multiple ‚Äúvirtual SerenaAgents‚Äù keyed by session.

Given the existing multi-project support and the new ‚Äúno process isolation needed‚Äù constraint, this is not insane, just a fair amount of work.

### 3.3 Interim compromise: one process per client type, not per window

Even before full multi-session:

- Run Serena once in HTTP mode for Claude Code.
- Keep the existing stdio integration for IDE plugins that don‚Äôt support HTTP yet.
- Within HTTP world, you still get ‚Äúone process, many chats‚Äù.

That already cuts your local process count massively if most of your usage is through Claude Code.

---

## 4. Totally outside-the-box options

A few other angles that might appeal to you as a ‚Äújack-of-all-trades‚Äù ops/dev:

1. **External supervisor for Serena instances** 
- Run all Serena instances under a `serena-supervisor` wrapper (Python or Go) that: 
- Tracks child PIDs, ports and projects.
- Exposes *its own* tiny HTTP API / UI (or even a TUI) to list and kill them.
- Wire clients to call that wrapper instead of `serena-mcp-server` directly.
- Benefit: No changes to Serena upstream, fully in your control.
2. **Systemd user service + HTTP-only Serena** 
- On your dev machines, run Serena as a **systemd user service** on a fixed port in HTTP mode.
- Configure *all* tools (Claude Code, ChatGPT, custom stuff) to talk to that single service.
- Add a tiny `serenactl` CLI to query status, tail logs, etc.
3. **Use a generic MCP proxy (VMCP) as the central hub** 
- There are emerging ‚Äúvirtual MCP‚Äù / proxy servers that multiplex between one client and many servers.
- You could flip it: use a proxy that presents **one** remote MCP to Claude Code, and under the hood: 
- Spins Serena instances lazily and kills them when idle.
- Reuses Serena per project instead of per conversation.
 That‚Äôs more for fun than practicality right now, but it aligns with where the ecosystem‚Äôs going.

---

## 5. If I were you, in what order would I attack this?

Realistically, in terms of ROI vs pain:

1. **Config tweak now** 
- Set `web_dashboard_open_on_launch: false` (+ possibly `web_dashboard: false` for pure IDE/Claude workflows).
- That alone makes the current behaviour much less obnoxious.
2. **Instance registry + multi-instance dashboard** 
- Minimal patch set touching only: 
- `serena/util/instance_registry.py` (new)
- `agent.py` (register/deregister)
- `dashboard.py` (routes + deregister on shutdown)
- `dashboard.js` + `index.html` (UI for all instances)
- Gives you the ‚Äúone dashboard to rule them all‚Äù you asked for, *without* touching MCP internals.
3. **Idle shutdown** 
- Simple watchdog in `SerenaMCPFactorySingleProcess` using a config value.
- Solves orphaned processes and memory bloat over long sessions.
4. **Optional: HTTP-only central server for your own setup** 
- Run Serena with `--transport streamable-http` as a single daemon.
- Configure Claude Code to use `--transport http` to that endpoint.
- Accept that, until you do a bigger refactor, active project is global, so you manually keep ‚Äúone main project‚Äù per server.
5. **Longer-term / upstream-worthy** 
- Proper multi-session project isolation for HTTP transports (session ‚Üí active project mapping).
- Better client docs showing how to use Serena as a *remote HTTP* MCP instead of stdio for devs who hate process spam.

---

If you want, next step we can sketch actual `InstanceRegistry` code and the dashboard API changes as a PR-ready patch set (filenames + rough diffs) so you can try it locally or upstream it.


---

## üë§ User

how about this then?

So instance registry is a great idea. I only want to make ADDITIVE changes (so i can keep the original repo as an upstream for updates etc). So rather than modifying the existing dashboard, lets ADD a new GLOBAL dashboard based on the instance registry. The global dashboard should:

- Replicate the existing dashboard but with a tabbed interface below the header. These sections: "Current Configuration", "Tool Usage", "Executions Queue", "Last Execution", "Registered Projects", "Available Tools", "Available Modes", "Available Contexts" should be grouped and shown in one tab per running instance.

- In my MCP configs, there should be new optional parameters called "web_dashboard_global", "web_dashboard_global_open_on_launch", "web_dashboard_global_port". This parameter should cause the web dashboard to open ONCE on the specified port (so if its already running then don‚Äôt try to run it again).

- Each tab should show The process id number as its name followed by the active project name (when activated) e.g. ‚Äú24284 - PROJECT_NAME‚Äù and should look like this if no active project: ‚Äú24284 - NO PROJECT‚Äù.- If possible turn a tab header yellow (live but no active project), Green (live with active project), Red (zombie process).

---

## ü§ñ Assistant

Yeah, this is a nice evolution of the idea, Mick. üëç
You get what you want (one global view, no invasive refactor) and we keep Serena‚Äôs existing dashboard intact.

I‚Äôll lay this out as something you can literally turn into a PR: new files + a couple of small hooks.

---

## 1. High-level shape

Add **three new pieces**, all additive:

1. **Instance registry** 
- New module: `serena/util/instance_registry.py`
- Tracks: `pid`, dashboard `port`, active project, context, modes, timestamps, etc.
- Backed by a JSON file under `~/.serena` (same area Serena already uses).  [Glama ‚Äì MCP Hosting Platform](https://glama.ai/mcp/servers/%40oraios/serena/blob/2ad1b7b31fdd4788a038fd528cbee701fff9c292/src/serena/constants.py)
2. **Global dashboard backend** 
- New module: `serena/global_dashboard.py` (similar to `serena/dashboard.py`).  [Glama ‚Äì MCP Hosting Platform](https://glama.ai/mcp/servers/%40oraios/serena/blob/2ad1b7b31fdd4788a038fd528cbee701fff9c292/src/serena/dashboard.py)
- Flask app serving: 
- Static front-end: `/global-dashboard/`
- API endpoints that **proxy** to the per-instance dashboards.
3. **Global dashboard frontend** 
- New directory: `src/serena/resources/global_dashboard/`  [Glama ‚Äì MCP Hosting Platform](https://glama.ai/mcp/servers/%40oraios/serena/blob/2ad1b7b31fdd4788a038fd528cbee701fff9c292/src/serena/constants.py?utm_source=chatgpt.com)
- `index.html` + `global_dashboard.js` = **copy of the current dashboard UI**, modified to: 
- Render a tab bar.
- For each tab, call the global API with a `pid` to get the per-instance data.

Existing dashboard (`/dashboard/index.html`) and `SerenaDashboardAPI` remain exactly as-is; we just reuse them.

---

## 2. Instance Registry (per-PID record of instances)

### Storage

Use the existing Serena managed dir:

```python
# serena/util/instance_registry.py
from __future__ import annotations

import json
import os
import time
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Optional, List, Dict

from serena.constants import SERENA_MANAGED_DIR_IN_HOME  # ~/.serena :contentReference[oaicite:3]{index=3}

_REGISTRY_PATH = Path(SERENA_MANAGED_DIR_IN_HOME) / "instances.json"
_REGISTRY_PATH.parent.mkdir(parents=True, exist_ok=True)

```

### Data model

```python
@dataclass
class InstanceInfo:
    pid: int
    port: int            # dashboard port
    started_at: float
    last_heartbeat: float
    context: Optional[str] = None
    modes: Optional[list[str]] = None
    project_name: Optional[str] = None
    state: str = "unknown"   # live_no_project / live_with_project / zombie

```

### Registry API (minimal)

```python
class InstanceRegistry:
    def __init__(self, path: Path = _REGISTRY_PATH):
        self._path = path

    def _load(self) -> Dict[int, InstanceInfo]:
        if not self._path.exists():
            return {}
        with self._path.open(encoding="utf-8") as f:
            raw = json.load(f)
        return {int(d["pid"]): InstanceInfo(**d) for d in raw}

    def _save(self, instances: Dict[int, InstanceInfo]) -> None:
        tmp = self._path.with_suffix(".tmp")
        with tmp.open("w", encoding="utf-8") as f:
            json.dump([asdict(i) for i in instances.values()], f)
        tmp.replace(self._path)

    def register_or_update(
        self,
        pid: int,
        port: int,
        *,
        context: Optional[str],
        modes: list[str],
    ) -> InstanceInfo:
        instances = self._load()
        now = time.time()
        info = instances.get(pid) or InstanceInfo(
            pid=pid,
            port=port,
            started_at=now,
            last_heartbeat=now,
            context=context,
            modes=modes,
        )
        info.port = port
        info.context = context
        info.modes = modes
        info.last_heartbeat = now
        instances[pid] = info
        self._save(instances)
        return info

    def update_project(self, pid: int, project_name: Optional[str]) -> None:
        instances = self._load()
        if pid in instances:
            instances[pid].project_name = project_name
            instances[pid].last_heartbeat = time.time()
            self._save(instances)

    def unregister(self, pid: int) -> None:
        instances = self._load()
        if pid in instances:
            del instances[pid]
            self._save(instances)

    def list_instances(self) -> list[InstanceInfo]:
        return list(self._load().values())

```

You *can* add actual locking later if you see races (e.g. using `filelock`), but for ‚Äúa few Serena instances starting now and then‚Äù this is fine.

---

## 3. Wire the registry into SerenaAgent (minimal, additive)

We only touch `SerenaAgent.__init__` and the project activation path, not the dashboard implementation.

### 3.1. After starting the per-instance dashboard

Right now, it does (simplified):  [Glama ‚Äì MCP Hosting Platform+1](https://glama.ai/mcp/servers/%40oraios/serena/blob/2ad1b7b31fdd4788a038fd528cbee701fff9c292/src/serena/agent.py)

```python
if self.serena_config.web_dashboard:
    self._dashboard_thread, port = SerenaDashboardAPI(
        get_memory_log_handler(), tool_names, agent=self, tool_usage_stats=self._tool_usage_stats
    ).run_in_thread()
    dashboard_url = f"http://127.0.0.1:{port}/dashboard/index.html"
    log.info("Serena web dashboard started at %s", dashboard_url)
    if self.serena_config.web_dashboard_open_on_launch:
        ...

```

Add just below that:

```python
from serena.util.instance_registry import InstanceRegistry

self._dashboard_port = port  # optional: keep handy
self._instance_registry = InstanceRegistry()
self._instance_registry.register_or_update(
    pid=os.getpid(),
    port=port,
    context=self._context.name if self._context else None,
    modes=[m.name for m in self._modes] if hasattr(self, "_modes") else [],
)

```

This gives you a registry entry for every Serena instance that has a dashboard.

### 3.2. Update registry when a project is activated

Wherever `activate_project_from_path_or_name` ends by setting `self._active_project` (see that function in `agent.py`), add a line to push project name into the registry.  [blog.csdn.net](https://blog.csdn.net/gitblog_00849/article/details/151252157?utm_source=chatgpt.com)

```python
self._active_project = project_instance
try:
    from serena.util.instance_registry import InstanceRegistry
    InstanceRegistry().update_project(os.getpid(), project_instance.project_name)
except Exception:
    log.debug("Could not update instance registry with project name", exc_info=True)

```

Again, totally additive.

### 3.3. Cleanup on shutdown

In `SerenaDashboardAPI._shutdown`, after logging and before/after killing the process, you *could* try to unregister:

```python
from serena.util.instance_registry import InstanceRegistry

def _shutdown(self) -> None:
    log.info("Shutting down Serena")
    try:
        InstanceRegistry().unregister(os.getpid())
    except Exception:
        log.debug("Could not unregister instance", exc_info=True)
    if self._shutdown_callback:
        self._shutdown_callback()
    else:
        os._exit(0)

```

If you don‚Äôt want to touch `dashboard.py` at all, you can skip this and let the global dashboard detect zombies heuristically (see section 6).

---

## 4. Global dashboard backend (serena/global_dashboard.py)

This is a sibling to `serena/dashboard.py`, not a replacement. It uses the registry and proxies requests down to each per-instance dashboard.

### 4.1. New constant

In `constants.py` you can (optionally) add, *additive*:

```python
SERENA_GLOBAL_DASHBOARD_DIR = str(_serena_pkg_path / "resources" / "global_dashboard")
SERENA_GLOBAL_DASHBOARD_PORT_DEFAULT = 25282  # deliberately outside 24282+ dynamic range :contentReference[oaicite:6]{index=6}

```

### 4.2. Global dashboard Flask app

Rough sketch:

```python
# serena/global_dashboard.py
import socket
import threading
import time
from typing import Any

import requests
from flask import Flask, Response, jsonify, request, send_from_directory
from pydantic import BaseModel

from serena.constants import SERENA_GLOBAL_DASHBOARD_DIR, SERENA_GLOBAL_DASHBOARD_PORT_DEFAULT
from serena.util.instance_registry import InstanceRegistry
from sensai.util import logging

log = logging.getLogger(__name__)


class InstanceSummary(BaseModel):
    pid: int
    port: int
    project_name: str | None
    context: str | None
    modes: list[str]
    state: str  # live_no_project / live_with_project / zombie


class SerenaGlobalDashboardAPI:
    def __init__(self, registry: InstanceRegistry | None = None) -> None:
        self._app = Flask(__name__)
        self._registry = registry or InstanceRegistry()
        self._setup_routes()

    def _setup_routes(self) -> None:
        app = self._app

        # Static UI
        @app.route("/global-dashboard/<path:filename>")
        def serve_dashboard(filename: str) -> Response:
            return send_from_directory(SERENA_GLOBAL_DASHBOARD_DIR, filename)

        @app.route("/global-dashboard/")
        def serve_dashboard_index() -> Response:
            return send_from_directory(SERENA_GLOBAL_DASHBOARD_DIR, "index.html")

        # Registry / summary
        @app.route("/global-dashboard/instances", methods=["GET"])
        def list_instances_route() -> dict[str, Any]:
            return {"instances": [s.model_dump() for s in self._build_instance_summaries()]}

        # Proxy endpoints (one set per instance)
        @app.route("/global-dashboard/instance/<int:pid>/get_log_messages", methods=["POST"])
        def proxy_get_log_messages(pid: int) -> dict[str, Any]:
            return self._proxy_to_instance(pid, "post", "/get_log_messages", json=request.get_json())

        @app.route("/global-dashboard/instance/<int:pid>/get_tool_names", methods=["GET"])
        def proxy_get_tool_names(pid: int) -> dict[str, Any]:
            return self._proxy_to_instance(pid, "get", "/get_tool_names")

        @app.route("/global-dashboard/instance/<int:pid>/get_tool_stats", methods=["GET"])
        def proxy_get_tool_stats(pid: int) -> dict[str, Any]:
            return self._proxy_to_instance(pid, "get", "/get_tool_stats")

        @app.route("/global-dashboard/instance/<int:pid>/clear_tool_stats", methods=["POST"])
        def proxy_clear_tool_stats(pid: int) -> dict[str, Any]:
            return self._proxy_to_instance(pid, "post", "/clear_tool_stats")

        @app.route("/global-dashboard/instance/<int:pid>/shutdown", methods=["PUT"])
        def proxy_shutdown(pid: int) -> dict[str, Any]:
            res = self._proxy_to_instance(pid, "put", "/shutdown")
            self._registry.unregister(pid)
            return res

    # Build tab metadata with health checks
    def _build_instance_summaries(self) -> list[InstanceSummary]:
        instances = self._registry.list_instances()
        summaries: list[InstanceSummary] = []
        now = time.time()

        for inst in instances:
            state = "zombie"
            project_name = inst.project_name
            try:
                r = requests.post(
                    f"http://127.0.0.1:{inst.port}/get_log_messages",
                    json={"start_idx": 0},
                    timeout=0.3,
                )
                if r.ok:
                    payload = r.json()
                    project_name = payload.get("active_project")
                    state = "live_with_project" if project_name else "live_no_project"
                    inst.project_name = project_name
                    inst.last_heartbeat = now
            except Exception:
                # unreachable => zombie
                pass

            inst.state = state
            summaries.append(
                InstanceSummary(
                    pid=inst.pid,
                    port=inst.port,
                    project_name=project_name,
                    context=inst.context,
                    modes=inst.modes or [],
                    state=state,
                )
            )

        return summaries

    def _proxy_to_instance(self, pid: int, method: str, path: str, **kwargs: Any) -> dict[str, Any]:
        inst_map = {i.pid: i for i in self._registry.list_instances()}
        inst = inst_map.get(pid)
        if inst is None:
            return {"error": f"Unknown instance {pid}"}
        url = f"http://127.0.0.1:{inst.port}{path}"
        r = requests.request(method, url, timeout=1.0, **kwargs)
        r.raise_for_status()
        return r.json()

    def run(self, host: str = "127.0.0.1", port: int = SERENA_GLOBAL_DASHBOARD_PORT_DEFAULT) -> None:
        from flask import cli
        cli.show_server_banner = lambda *args, **kwargs: None  # mirror normal dashboard behaviour :contentReference[oaicite:7]{index=7}
        self._app.run(host=host, port=port, debug=False, use_reloader=False, threaded=True)

    def run_in_thread_if_port_free(self, port: int) -> tuple[threading.Thread | None, int | None]:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            sock.bind(("127.0.0.1", port))
        except OSError:
            # someone else is already running the global dashboard
            sock.close()
            return None, None
        sock.close()

        thread = threading.Thread(target=lambda: self.run(port=port), daemon=True)
        thread.start()
        return thread, port

```

No changes to the existing `SerenaDashboardAPI`.

---

## 5. Config additions & ‚Äúopen once‚Äù semantics

Serena‚Äôs config base class already has `web_dashboard` and `web_dashboard_open_on_launch`.  [Glama ‚Äì MCP Hosting Platform+1](https://glama.ai/mcp/servers/%40lin2000wl/Serena-cursor-mcp/blob/5c077c503c741267f1e99ae3f8c3507912100618/src/serena/agent.py?utm_source=chatgpt.com)

### 5.1. Extend SerenaConfigBase

In whatever file defines `SerenaConfigBase` (the snippet in the Agno repo shows it), add:

```python
@dataclass(kw_only=True)
class SerenaConfigBase(ABC):
    ...
    web_dashboard: bool = True
    web_dashboard_open_on_launch: bool = True
    # NEW:
    web_dashboard_global: bool = False
    web_dashboard_global_open_on_launch: bool = False
    web_dashboard_global_port: int | None = None
    ...

```

Defaults keep current behaviour exactly.

### 5.2. Template (serena_config.template.yml)

In `src/serena/resources/serena_config.template.yml`, add something like:  [GitHub](https://github.com/oraios/serena/discussions/357?utm_source=chatgpt.com)

```yaml
web_dashboard: true
web_dashboard_open_on_launch: true

# Global dashboard (aggregates all running Serena instances on a single page)
web_dashboard_global: false
web_dashboard_global_open_on_launch: false   # open the global view once when the first instance starts it
web_dashboard_global_port: 25282             # optional; default if omitted

```

### 5.3. Wiring it in SerenaAgent.__init__

After you register the instance in the registry, you handle the global dashboard:

```python
from serena.global_dashboard import SerenaGlobalDashboardAPI
from serena.constants import SERENA_GLOBAL_DASHBOARD_PORT_DEFAULT

...

self._global_dashboard_thread = None
if self.serena_config.web_dashboard_global:
    global_port = self.serena_config.web_dashboard_global_port or SERENA_GLOBAL_DASHBOARD_PORT_DEFAULT
    global_dashboard = SerenaGlobalDashboardAPI(self._instance_registry)
    thread, used_port = global_dashboard.run_in_thread_if_port_free(global_port)

    if thread is not None:
        self._global_dashboard_thread = thread
        global_url = f"http://127.0.0.1:{used_port}/global-dashboard/index.html"
        log.info("Serena global dashboard started at %s", global_url)
        if self.serena_config.web_dashboard_global_open_on_launch:
            process = multiprocessing.Process(target=self._open_dashboard, args=(global_url,))
            process.start()
            process.join(timeout=1)

```

Semantics:

- Every instance *tries* to start the global dashboard.
- Only the first one that can bind `web_dashboard_global_port` succeeds.
- Only that ‚Äúowner‚Äù instance opens the browser (once).
- Later instances do nothing extra‚Äîexactly what you asked for.

You can also choose to set `web_dashboard: false` and only use the global dashboard (if you‚Äôre happy with a single UI).

---

## 6. Frontend: tabbed global dashboard

### 6.1. Files

Create `src/serena/resources/global_dashboard/`  [Glama ‚Äì MCP Hosting Platform+1](https://glama.ai/mcp/servers/%40oraios/serena/blob/2ad1b7b31fdd4788a038fd528cbee701fff9c292/src/serena/constants.py?utm_source=chatgpt.com) with at least:

- `index.html`
- `global_dashboard.js`
- Optionally reuse CSS/assets from `resources/dashboard`.

Easiest: copy `resources/dashboard/index.html` to `global_dashboard/index.html` and adjust the script include to `global_dashboard.js`.

### 6.2. Data flow

On load:

1. `GET /global-dashboard/instances` 
- Returns an array of `InstanceSummary { pid, port, project_name, context, modes, state }`.
2. Render a tab bar based on these instances.
3. When a tab is selected: 
- Use the **proxy endpoints** instead of the original ones:
 
```javascript
const base = `/global-dashboard/instance/${pid}`;

// Instead of fetch('/get_log_messages'...), do:
fetch(`${base}/get_log_messages`, {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ start_idx: currentLogCursor[pid] || 0 })
})

// Likewise for:
//   `${base}/get_tool_names`
//   `${base}/get_tool_stats`
//   `${base}/clear_tool_stats`
//   `${base}/shutdown`

```

Because the browser only calls `http://127.0.0.1:<GLOBAL_PORT>/...`, no CORS problems ‚Äî the Flask backend talks to each instance on 24282+ and just returns JSON.

### 6.3. Tab titles & colors

For each `InstanceSummary`:

- Label logic: 
```javascript
function instanceLabel(inst) {
  const name = inst.project_name || 'NO PROJECT';
  return `${inst.pid} - ${name}`;
}

```
- State ‚Üí color: 
```javascript
function tabClass(inst) {
  switch (inst.state) {
    case 'live_with_project':
      return 'tab tab-green';
    case 'live_no_project':
      return 'tab tab-yellow';
    case 'zombie':
    default:
      return 'tab tab-red';
  }
}

```

CSS (example):

```css
.tab-bar {
  display: flex;
  gap: 0.5rem;
  margin-bottom: 0.75rem;
}

.tab {
  padding: 0.25rem 0.75rem;
  border-radius: 4px;
  cursor: pointer;
  font-family: monospace;
}

.tab-green { background: #d1fae5; border: 1px solid #059669; }  /* live + project */
.tab-yellow { background: #fef3c7; border: 1px solid #d97706; }/* live, no project */
.tab-red { background: #fee2e2; border: 1px solid #b91c1c; }   /* zombie */
.tab-active { box-shadow: 0 0 0 2px rgba(59,130,246,0.5); }

```

### 6.4. Sections per tab

You want the same sections as the existing dashboard, grouped *per instance*:

- Current Configuration
- Tool Usage
- Executions Queue
- Last Execution
- Registered Projects
- Available Tools
- Available Modes
- Available Contexts

The easiest mental model:

- Keep the same HTML structure **inside a container** that is swapped based on active tab.
- Instead of a global `state`, have a `perPidState[pid]` in JS and render based on the currently active pid.

You can basically lift most of the existing JS, pass in a `pid` and route all fetches through the proxy paths above.

---

## 7. Zombie detection and status semantics

You asked for:

- Yellow ‚Äî ‚Äúlive but no active project‚Äù.
- Green ‚Äî ‚Äúlive with active project‚Äù.
- Red ‚Äî ‚Äúzombie process‚Äù.

The backend already has enough info:

- `get_log_messages` response has `active_project` field.  [Glama ‚Äì MCP Hosting Platform+1](https://glama.ai/mcp/servers/%40oraios/serena/blob/2ad1b7b31fdd4788a038fd528cbee701fff9c292/src/serena/dashboard.py?utm_source=chatgpt.com)
- If a dashboard doesn‚Äôt respond on its port within a short timeout ‚Üí zombie.

In `_build_instance_summaries` (above) we already do:

- If request works: 
- If `active_project` truthy ‚Üí `state = live_with_project` (green).
- Else ‚Üí `state = live_no_project` (yellow).
- If request fails or connection refused ‚Üí `state = zombie` (red).

You can optionally:

- Auto-prune zombies that have been `zombie` for > N minutes from the registry.
- Add a ‚ÄúKill‚Äù button that calls the `/shutdown` proxy (best effort cleanup).

---

## 8. Summary of actual code surface you touch

**New files (all additive):**

- `src/serena/util/instance_registry.py`
- `src/serena/global_dashboard.py`
- `src/serena/resources/global_dashboard/index.html`
- `src/serena/resources/global_dashboard/global_dashboard.js`
- Optional: small CSS file or reuse existing dashboard CSS.

**Small additive changes to existing files:**

- `src/serena/constants.py` 
- Add `SERENA_GLOBAL_DASHBOARD_DIR` and default port constant.
- `src/serena/config/serena_config.py` (where `SerenaConfigBase` lives) 
- Add `web_dashboard_global`, `web_dashboard_global_open_on_launch`, `web_dashboard_global_port`.
- `src/serena/resources/serena_config.template.yml` 
- Add the three new config keys.
- `src/serena/agent.py` 
- After dashboard start: register instance with `InstanceRegistry`.
- In project activation: update `project_name` in registry.
- After that: conditionally start global dashboard once based on `web_dashboard_global*`.
- (Optional but nice) `src/serena/dashboard.py` 
- On `_shutdown`, call `InstanceRegistry().unregister(os.getpid())`.

All of this keeps upstream‚Äôs behaviour intact unless someone opts into the new knobs.

---

If you want, next step I can sketch the **exact** JS changes to clone the current dashboard into a tab-aware `global_dashboard.js` so you can pretty much paste it in and iterate locally.


---
